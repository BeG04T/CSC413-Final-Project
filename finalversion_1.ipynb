{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6fZPI0rXy2aD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files, drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJo7RSLLy2aF",
        "outputId": "b1a06240-176c-499f-9056-ca4a9b0bbbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download(['stopwords', 'wordnet', 'punkt', ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bGnc5wfy2aF",
        "outputId": "51421e13-6871-4e4b-9d19-56f7ed08384a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.is_available()\n",
        "drive.mount('/content/gdrive')\n",
        "short_stories_filename = \"/content/gdrive/My Drive/Colab Notebooks/CSC413/Project/reddit_short_stories.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l4otQjYTy2aG"
      },
      "outputs": [],
      "source": [
        "with open(short_stories_filename, 'r') as file:\n",
        "    data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qG9MYRWGy2aG"
      },
      "outputs": [],
      "source": [
        "# Split the data string into a list of stories\n",
        "stories = data.split(\"<eos>\\n<sos>\")\n",
        "\n",
        "# Remove the first <sos> tag from the first story\n",
        "stories[0] = stories[0].replace(\"<sos>\", \"\")\n",
        "\n",
        "# Remove the last <eos> tag from the last story\n",
        "stories[-1] = stories[-1].replace(\"<eos>\", \"\")\n",
        "\n",
        "# Create a dataframe with one story per row\n",
        "reddit_df = pd.DataFrame(stories, columns=['contents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TQpgjR5Py2aG"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[{}]+'.format(string.punctuation), ' ', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "def strip_spaces(text):\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "def clean_sentence(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(re.compile('<.*?>'), '', text)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = remove_punctuation(text)\n",
        "    \n",
        "    # Remove stop words\n",
        "    text = remove_stopwords(text.lower())\n",
        "\n",
        "    # Lemmatize words in the sentence\n",
        "    text = lemmatize_sentence(text)\n",
        "\n",
        "    # Strip spaces\n",
        "    text = strip_spaces(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_ngrams(sentence, n_grams):\n",
        "    tokenized = nltk.word_tokenize(sentence.lower())\n",
        "    return list(ngrams(tokenized, n_grams))\n",
        "\n",
        "def clean_story(text):\n",
        "    sentences = []\n",
        "    for sentence in text.split('.'):\n",
        "        if sentence:\n",
        "            sentences.append(clean_sentence(sentence))\n",
        "    return sentences\n",
        "\n",
        "def generate_n_grams(text, n_grams: int = 3):\n",
        "    arr = []\n",
        "    for sentence in text:\n",
        "        arr += create_ngrams(sentence, n_grams)\n",
        "    return arr "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ALq57xb1y2aH"
      },
      "outputs": [],
      "source": [
        "# Decrease amount of stories as dataset is very large\n",
        "reddit_df = reddit_df.iloc[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DpIlPH3cy2aH"
      },
      "outputs": [],
      "source": [
        "# Perform story cleaning\n",
        "reddit_df['sentences'] = reddit_df['contents'].apply(lambda text: clean_story(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mmn6furVy2aI"
      },
      "outputs": [],
      "source": [
        "# Generate ngrams\n",
        "reddit_df['n_grams'] = reddit_df['sentences'].apply(lambda text: generate_n_grams(text, n_grams=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jUXEkcqoy2aI"
      },
      "outputs": [],
      "source": [
        "n_grams = np.concatenate(reddit_df['n_grams'].values).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HLSWTivUy2aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6a209c-50b0-46f9-85e8-85d221b21be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['learned', 'name', 'phonebook', 'shaking', 'finger'],\n",
              " ['name', 'phonebook', 'shaking', 'finger', 'carefully'],\n",
              " ['phonebook', 'shaking', 'finger', 'carefully', 'caressing'],\n",
              " ['shaking', 'finger', 'carefully', 'caressing', 'page'],\n",
              " ['finger', 'carefully', 'caressing', 'page', 'searched'],\n",
              " ['carefully', 'caressing', 'page', 'searched', 'address'],\n",
              " ['caressing', 'page', 'searched', 'address', 'seen'],\n",
              " ['page', 'searched', 'address', 'seen', 'many'],\n",
              " ['searched', 'address', 'seen', 'many', 'time'],\n",
              " ['43', 'mako', 'drive', 'small', 'brick'],\n",
              " ['mako', 'drive', 'small', 'brick', 'house'],\n",
              " ['drive', 'small', 'brick', 'house', 'corner'],\n",
              " ['small', 'brick', 'house', 'corner', 'braxton'],\n",
              " ['brick', 'house', 'corner', 'braxton', 'mako'],\n",
              " ['memorized', 'shape', 'home', 'week', 'bare'],\n",
              " ['shape', 'home', 'week', 'bare', 'foot'],\n",
              " ['home', 'week', 'bare', 'foot', 'sliding'],\n",
              " ['week', 'bare', 'foot', 'sliding', 'across'],\n",
              " ['bare', 'foot', 'sliding', 'across', 'wet'],\n",
              " ['foot', 'sliding', 'across', 'wet', 'grass'],\n",
              " ['sliding', 'across', 'wet', 'grass', 'every'],\n",
              " ['across', 'wet', 'grass', 'every', 'time'],\n",
              " ['wet', 'grass', 'every', 'time', 'closed'],\n",
              " ['grass', 'every', 'time', 'closed', 'eye'],\n",
              " ['letter', 'mailbox', 'addressed', 'caroline', 'smith'],\n",
              " ['mailbox', 'addressed', 'caroline', 'smith', 'confirmed'],\n",
              " ['addressed', 'caroline', 'smith', 'confirmed', 'book'],\n",
              " ['caroline', 'smith', 'confirmed', 'book', 'claimed'],\n",
              " ['perfect', 'absolutely', 'flawless', 'every', 'way'],\n",
              " ['watched', 'seven', 'month', 'almost', 'every'],\n",
              " ['seven', 'month', 'almost', 'every', 'single'],\n",
              " ['month', 'almost', 'every', 'single', 'day'],\n",
              " ['almost', 'every', 'single', 'day', 'night'],\n",
              " ['every', 'single', 'day', 'night', 'silently'],\n",
              " ['single', 'day', 'night', 'silently', 'following'],\n",
              " ['day', 'night', 'silently', 'following', 'strolled'],\n",
              " ['night', 'silently', 'following', 'strolled', 'class'],\n",
              " ['sometimes', 'stayed', 'outside', 'bedroom', 'window'],\n",
              " ['stayed', 'outside', 'bedroom', 'window', 'slept'],\n",
              " ['outside', 'bedroom', 'window', 'slept', 'heart'],\n",
              " ['bedroom', 'window', 'slept', 'heart', 'racing'],\n",
              " ['window', 'slept', 'heart', 'racing', 'matched'],\n",
              " ['slept', 'heart', 'racing', 'matched', 'breathing'],\n",
              " ['never', 'knew', 'never', 'acknowledged', 'distant'],\n",
              " ['knew', 'never', 'acknowledged', 'distant', 'shadow'],\n",
              " ['never', 'acknowledged', 'distant', 'shadow', 'faceless'],\n",
              " ['acknowledged', 'distant', 'shadow', 'faceless', 'tree'],\n",
              " ['distant', 'shadow', 'faceless', 'tree', 'knew'],\n",
              " ['shadow', 'faceless', 'tree', 'knew', 'needed'],\n",
              " ['anything', 'could', 'convince', 'angel', 'truly'],\n",
              " ['could', 'convince', 'angel', 'truly', 'visited'],\n",
              " ['convince', 'angel', 'truly', 'visited', 'greasy'],\n",
              " ['angel', 'truly', 'visited', 'greasy', 'obscene'],\n",
              " ['truly', 'visited', 'greasy', 'obscene', 'vile'],\n",
              " ['visited', 'greasy', 'obscene', 'vile', 'planet'],\n",
              " ['greasy', 'obscene', 'vile', 'planet', 'would'],\n",
              " ['obscene', 'vile', 'planet', 'would', 'see'],\n",
              " ['artist', 'creator', 'built', 'perfect', 'world'],\n",
              " ['creator', 'built', 'perfect', 'world', 'could'],\n",
              " ['built', 'perfect', 'world', 'could', 'appreciate'],\n",
              " ['perfect', 'world', 'could', 'appreciate', 'universe'],\n",
              " ['world', 'could', 'appreciate', 'universe', 'fit'],\n",
              " ['could', 'appreciate', 'universe', 'fit', 'two'],\n",
              " ['appreciate', 'universe', 'fit', 'two', 'u'],\n",
              " ['taught', 'art', 'community', 'center', 'next'],\n",
              " ['art', 'community', 'center', 'next', 'unsightly'],\n",
              " ['community', 'center', 'next', 'unsightly', 'yellow'],\n",
              " ['center', 'next', 'unsightly', 'yellow', 'pizza'],\n",
              " ['next', 'unsightly', 'yellow', 'pizza', 'restaurant'],\n",
              " ['student', 'respect', 'teacher', 'understand', 'one'],\n",
              " ['respect', 'teacher', 'understand', 'one', 'truly'],\n",
              " ['teacher', 'understand', 'one', 'truly', 'valued'],\n",
              " ['see', 'perfection', 'talent', 'unearthly', 'skill'],\n",
              " ['perfection', 'talent', 'unearthly', 'skill', 'possessed'],\n",
              " ['world', 'able', 'comprehend', 'could', 'god'],\n",
              " ['able', 'comprehend', 'could', 'god', 'could'],\n",
              " ['comprehend', 'could', 'god', 'could', 'fathom'],\n",
              " ['could', 'god', 'could', 'fathom', 'beauty'],\n",
              " ['knew', 'free', 'save', 'life', 'dismay'],\n",
              " ['free', 'save', 'life', 'dismay', 'disrespect'],\n",
              " ['save', 'life', 'dismay', 'disrespect', 'endured'],\n",
              " ['always', 'walked', 'alone', 'always', 'spent'],\n",
              " ['walked', 'alone', 'always', 'spent', 'day'],\n",
              " ['alone', 'always', 'spent', 'day', 'night'],\n",
              " ['always', 'spent', 'day', 'night', 'paintbrush'],\n",
              " ['spent', 'day', 'night', 'paintbrush', 'canvas'],\n",
              " ['mail', '43', 'mako', 'drive', 'never'],\n",
              " ['43', 'mako', 'drive', 'never', 'addressed'],\n",
              " ['mako', 'drive', 'never', 'addressed', 'anyone'],\n",
              " ['drive', 'never', 'addressed', 'anyone', 'caroline'],\n",
              " ['never', 'addressed', 'anyone', 'caroline', 'finger'],\n",
              " ['addressed', 'anyone', 'caroline', 'finger', 'becoming'],\n",
              " ['anyone', 'caroline', 'finger', 'becoming', 'accustomed'],\n",
              " ['caroline', 'finger', 'becoming', 'accustomed', 'rub'],\n",
              " ['finger', 'becoming', 'accustomed', 'rub', 'ink'],\n",
              " ['becoming', 'accustomed', 'rub', 'ink', 'stained'],\n",
              " ['accustomed', 'rub', 'ink', 'stained', 'c'],\n",
              " ['rub', 'ink', 'stained', 'c', 'name'],\n",
              " ['ink', 'stained', 'c', 'name', 'pressed'],\n",
              " ['stained', 'c', 'name', 'pressed', 'envelope'],\n",
              " ['one', 'art', 'nothing', 'world', 'created'],\n",
              " ['art', 'nothing', 'world', 'created', 'comfort'],\n",
              " ['nothing', 'world', 'created', 'comfort', 'home'],\n",
              " ['world', 'created', 'comfort', 'home', 'silently'],\n",
              " ['created', 'comfort', 'home', 'silently', 'watched'],\n",
              " ['comfort', 'home', 'silently', 'watched', 'shroud'],\n",
              " ['home', 'silently', 'watched', 'shroud', 'long'],\n",
              " ['silently', 'watched', 'shroud', 'long', 'set'],\n",
              " ['watched', 'shroud', 'long', 'set', 'sun'],\n",
              " ['support', 'devotion', 'undying', 'love', 'admiration'],\n",
              " ['devotion', 'undying', 'love', 'admiration', 'yet'],\n",
              " ['undying', 'love', 'admiration', 'yet', 'knew'],\n",
              " ['love', 'admiration', 'yet', 'knew', 'enough'],\n",
              " ['needed', 'needed', 'embrace', 'planet', 'screamed'],\n",
              " ['needed', 'embrace', 'planet', 'screamed', 'name'],\n",
              " ['embrace', 'planet', 'screamed', 'name', 'singularity'],\n",
              " ['planet', 'screamed', 'name', 'singularity', 'hung'],\n",
              " ['screamed', 'name', 'singularity', 'hung', 'portrait'],\n",
              " ['name', 'singularity', 'hung', 'portrait', 'gallery'],\n",
              " ['singularity', 'hung', 'portrait', 'gallery', 'travelled'],\n",
              " ['hung', 'portrait', 'gallery', 'travelled', 'halfway'],\n",
              " ['portrait', 'gallery', 'travelled', 'halfway', 'around'],\n",
              " ['gallery', 'travelled', 'halfway', 'around', 'world'],\n",
              " ['travelled', 'halfway', 'around', 'world', 'admire'],\n",
              " ['halfway', 'around', 'world', 'admire', 'brush'],\n",
              " ['around', 'world', 'admire', 'brush', 'stroke'],\n",
              " ['needed', 'fame', 'fortune', 'acclaim', 'respect'],\n",
              " ['fame', 'fortune', 'acclaim', 'respect', 'follower'],\n",
              " ['fortune', 'acclaim', 'respect', 'follower', 'immorality'],\n",
              " ['knew', 'could', 'give', 'make', 'name'],\n",
              " ['could', 'give', 'make', 'name', 'commodity'],\n",
              " ['give', 'make', 'name', 'commodity', 'brand'],\n",
              " ['make', 'name', 'commodity', 'brand', 'u'],\n",
              " ['name', 'commodity', 'brand', 'u', 'single'],\n",
              " ['commodity', 'brand', 'u', 'single', 'entity'],\n",
              " ['brand', 'u', 'single', 'entity', 'history'],\n",
              " ['u', 'single', 'entity', 'history', 'humanity'],\n",
              " ['wanted', 'one', 'launch', 'fame', 'name'],\n",
              " ['one', 'launch', 'fame', 'name', 'always'],\n",
              " ['launch', 'fame', 'name', 'always', 'followed'],\n",
              " ['fame', 'name', 'always', 'followed', 'around'],\n",
              " ['wanted', 'reason', 'went', 'missing', 'person'],\n",
              " ['reason', 'went', 'missing', 'person', 'force'],\n",
              " ['went', 'missing', 'person', 'force', 'world'],\n",
              " ['needed', 'free', 'filthy', 'planet', 'one'],\n",
              " ['free', 'filthy', 'planet', 'one', 'release'],\n",
              " ['filthy', 'planet', 'one', 'release', 'soul'],\n",
              " ['planet', 'one', 'release', 'soul', 'million'],\n",
              " ['one', 'release', 'soul', 'million', 'scattered'],\n",
              " ['release', 'soul', 'million', 'scattered', 'throughout'],\n",
              " ['soul', 'million', 'scattered', 'throughout', 'corner'],\n",
              " ['million', 'scattered', 'throughout', 'corner', 'uncivilized'],\n",
              " ['scattered', 'throughout', 'corner', 'uncivilized', 'obscene'],\n",
              " ['throughout', 'corner', 'uncivilized', 'obscene', 'earth'],\n",
              " ['knew', 'could', 'inspire', 'mass', 'provoke'],\n",
              " ['could', 'inspire', 'mass', 'provoke', 'future'],\n",
              " ['left', 'alone', 'one', 'night', 'let'],\n",
              " ['alone', 'one', 'night', 'let', 'sleep'],\n",
              " ['one', 'night', 'let', 'sleep', 'without'],\n",
              " ['night', 'let', 'sleep', 'without', 'comfort'],\n",
              " ['let', 'sleep', 'without', 'comfort', 'warm'],\n",
              " ['sleep', 'without', 'comfort', 'warm', 'carcass'],\n",
              " ['without', 'comfort', 'warm', 'carcass', 'nestled'],\n",
              " ['comfort', 'warm', 'carcass', 'nestled', 'foot'],\n",
              " ['warm', 'carcass', 'nestled', 'foot', 'away'],\n",
              " ['soon', 'time', 'moment', 'forever', 'become'],\n",
              " ['time', 'moment', 'forever', 'become', 'name'],\n",
              " ['moment', 'forever', 'become', 'name', 'tied'],\n",
              " ['forever', 'become', 'name', 'tied', 'together'],\n",
              " ['become', 'name', 'tied', 'together', 'medium'],\n",
              " ['name', 'tied', 'together', 'medium', 'voice'],\n",
              " ['tied', 'together', 'medium', 'voice', 'people'],\n",
              " ['together', 'medium', 'voice', 'people', 'page'],\n",
              " ['medium', 'voice', 'people', 'page', 'history'],\n",
              " ['voice', 'people', 'page', 'history', 'world'],\n",
              " ['people', 'page', 'history', 'world', 'alike'],\n",
              " ['wanted', 'perfect', 'take', 'free', 'soul'],\n",
              " ['perfect', 'take', 'free', 'soul', 'immortality'],\n",
              " ['needed', 'flawless', 'enough', 'display', 'art'],\n",
              " ['flawless', 'enough', 'display', 'art', 'world'],\n",
              " ['prepped', 'painted', 'cleaned', 'set', 'forth'],\n",
              " ['painted', 'cleaned', 'set', 'forth', 'tool'],\n",
              " ['cleaned', 'set', 'forth', 'tool', 'extract'],\n",
              " ['set', 'forth', 'tool', 'extract', 'memory'],\n",
              " ['forth', 'tool', 'extract', 'memory', 'became'],\n",
              " ['tool', 'extract', 'memory', 'became', 'blurred'],\n",
              " ['extract', 'memory', 'became', 'blurred', 'uncertain'],\n",
              " ['memory', 'became', 'blurred', 'uncertain', 'toiled'],\n",
              " ['became', 'blurred', 'uncertain', 'toiled', 'endlessly'],\n",
              " ['time', 'content', 'eye', 'become', 'bloody'],\n",
              " ['content', 'eye', 'become', 'bloody', 'lack'],\n",
              " ['eye', 'become', 'bloody', 'lack', 'sleep'],\n",
              " ['become', 'bloody', 'lack', 'sleep', 'sun'],\n",
              " ['bloody', 'lack', 'sleep', 'sun', 'long'],\n",
              " ['lack', 'sleep', 'sun', 'long', 'since'],\n",
              " ['sleep', 'sun', 'long', 'since', 'risen'],\n",
              " ['room', 'bare', 'foot', 'touched', 'familiar'],\n",
              " ['bare', 'foot', 'touched', 'familiar', 'grass'],\n",
              " ['foot', 'touched', 'familiar', 'grass', 'outside'],\n",
              " ['touched', 'familiar', 'grass', 'outside', 'window'],\n",
              " ['pulled', 'open', 'unlocked', 'back', 'door'],\n",
              " ['open', 'unlocked', 'back', 'door', 'silently'],\n",
              " ['unlocked', 'back', 'door', 'silently', 'dragging'],\n",
              " ['back', 'door', 'silently', 'dragging', 'heel'],\n",
              " ['door', 'silently', 'dragging', 'heel', 'across'],\n",
              " ['silently', 'dragging', 'heel', 'across', 'hardwood'],\n",
              " ['dragging', 'heel', 'across', 'hardwood', 'floor'],\n",
              " ['heel', 'across', 'hardwood', 'floor', 'felt'],\n",
              " ['across', 'hardwood', 'floor', 'felt', 'many'],\n",
              " ['hardwood', 'floor', 'felt', 'many', 'time'],\n",
              " ['danced', 'spot', 'foot', 'softly', 'tapping'],\n",
              " ['spot', 'foot', 'softly', 'tapping', 'ground'],\n",
              " ['foot', 'softly', 'tapping', 'ground', 'inch'],\n",
              " ['softly', 'tapping', 'ground', 'inch', 'slept'],\n",
              " ['tapping', 'ground', 'inch', 'slept', 'could'],\n",
              " ['ground', 'inch', 'slept', 'could', 'hear'],\n",
              " ['inch', 'slept', 'could', 'hear', 'breathing'],\n",
              " ['slept', 'could', 'hear', 'breathing', 'perfect'],\n",
              " ['could', 'hear', 'breathing', 'perfect', 'synchronization'],\n",
              " ['hear', 'breathing', 'perfect', 'synchronization', 'spun'],\n",
              " ['bed', 'empty', 'window', 'shattered', 'shimmering'],\n",
              " ['empty', 'window', 'shattered', 'shimmering', 'atop'],\n",
              " ['window', 'shattered', 'shimmering', 'atop', 'sheet'],\n",
              " ['bureau', 'lay', 'sideways', 'content', 'spilled'],\n",
              " ['lay', 'sideways', 'content', 'spilled', 'floor'],\n",
              " ['picked', 'ruby', 'shirt', 'wore', 'bed'],\n",
              " ['ruby', 'shirt', 'wore', 'bed', 'almost'],\n",
              " ['shirt', 'wore', 'bed', 'almost', 'every'],\n",
              " ['wore', 'bed', 'almost', 'every', 'night'],\n",
              " ['bed', 'almost', 'every', 'night', 'held'],\n",
              " ['almost', 'every', 'night', 'held', 'face'],\n",
              " ['every', 'night', 'held', 'face', 'familiar'],\n",
              " ['night', 'held', 'face', 'familiar', 'scent'],\n",
              " ['held', 'face', 'familiar', 'scent', 'perfume'],\n",
              " ['face', 'familiar', 'scent', 'perfume', 'washing'],\n",
              " ['watch', 'spent', 'hour', 'sometimes', 'entire'],\n",
              " ['spent', 'hour', 'sometimes', 'entire', 'day'],\n",
              " ['hour', 'sometimes', 'entire', 'day', 'washing'],\n",
              " ['sometimes', 'entire', 'day', 'washing', 'organizing'],\n",
              " ['entire', 'day', 'washing', 'organizing', 'every'],\n",
              " ['day', 'washing', 'organizing', 'every', 'inch'],\n",
              " ['washing', 'organizing', 'every', 'inch', 'home'],\n",
              " ['organizing', 'every', 'inch', 'home', 'always'],\n",
              " ['every', 'inch', 'home', 'always', 'perfection'],\n",
              " ['mess', 'chaotic', 'wreck', 'turmoil', 'struggle'],\n",
              " ['never', 'done', 'never', 'forced', 'see'],\n",
              " ['done', 'never', 'forced', 'see', 'shape'],\n",
              " ['never', 'forced', 'see', 'shape', 'sheer'],\n",
              " ['forced', 'see', 'shape', 'sheer', 'humanity'],\n",
              " ['wall', 'rife', 'beauty', 'life', 'painted'],\n",
              " ['rife', 'beauty', 'life', 'painted', 'lay'],\n",
              " ['beauty', 'life', 'painted', 'lay', 'bare'],\n",
              " ['life', 'painted', 'lay', 'bare', 'art'],\n",
              " ['painted', 'lay', 'bare', 'art', 'scattered'],\n",
              " ['lay', 'bare', 'art', 'scattered', 'broken'],\n",
              " ['bare', 'art', 'scattered', 'broken', 'upon'],\n",
              " ['art', 'scattered', 'broken', 'upon', 'floor'],\n",
              " ['clenched', 'teeth', 'righted', 'muscle', 'tensing'],\n",
              " ['teeth', 'righted', 'muscle', 'tensing', 'tried'],\n",
              " ['righted', 'muscle', 'tensing', 'tried', 'hang'],\n",
              " ['muscle', 'tensing', 'tried', 'hang', 'back'],\n",
              " ['tensing', 'tried', 'hang', 'back', 'correct'],\n",
              " ['tried', 'hang', 'back', 'correct', 'place'],\n",
              " ['hang', 'back', 'correct', 'place', 'simply'],\n",
              " ['let', 'someone', 'else', 'touch', 'lost'],\n",
              " ['someone', 'else', 'touch', 'lost', 'perfection'],\n",
              " ['allowed', 'fall', 'back', 'floor', 'continued'],\n",
              " ['fall', 'back', 'floor', 'continued', 'stair'],\n",
              " ['creme', 'carpet', 'outside', 'studio', 'door'],\n",
              " ['carpet', 'outside', 'studio', 'door', 'stained'],\n",
              " ['outside', 'studio', 'door', 'stained', 'ruby'],\n",
              " ['studio', 'door', 'stained', 'ruby', 'red'],\n",
              " ['door', 'stained', 'ruby', 'red', 'still'],\n",
              " ['stained', 'ruby', 'red', 'still', 'moist'],\n",
              " ['ruby', 'red', 'still', 'moist', 'weight'],\n",
              " ['red', 'still', 'moist', 'weight', 'bare'],\n",
              " ['still', 'moist', 'weight', 'bare', 'foot'],\n",
              " ['could', 'hear', 'breathing', 'heavily', 'behind'],\n",
              " ['hear', 'breathing', 'heavily', 'behind', 'gasp'],\n",
              " ['breathing', 'heavily', 'behind', 'gasp', 'raspy'],\n",
              " ['heavily', 'behind', 'gasp', 'raspy', 'strained'],\n",
              " ['behind', 'gasp', 'raspy', 'strained', 'tremendous'],\n",
              " ['gasp', 'raspy', 'strained', 'tremendous', 'weight'],\n",
              " ['wrapped', 'hand', 'around', 'doorknob', 'twisting'],\n",
              " ['hand', 'around', 'doorknob', 'twisting', 'cold'],\n",
              " ['around', 'doorknob', 'twisting', 'cold', 'brass'],\n",
              " ['doorknob', 'twisting', 'cold', 'brass', 'knob'],\n",
              " ['twisting', 'cold', 'brass', 'knob', 'silently'],\n",
              " ['cold', 'brass', 'knob', 'silently', 'pushing'],\n",
              " ['brass', 'knob', 'silently', 'pushing', 'open'],\n",
              " ['blink', 'peered', 'vulgarity', 'exposed', 'almost'],\n",
              " ['peered', 'vulgarity', 'exposed', 'almost', 'unbearable'],\n",
              " ['room', 'disarray', 'painting', 'torn', 'apart'],\n",
              " ['disarray', 'painting', 'torn', 'apart', 'brush'],\n",
              " ['painting', 'torn', 'apart', 'brush', 'scattered'],\n",
              " ['torn', 'apart', 'brush', 'scattered', 'across'],\n",
              " ['apart', 'brush', 'scattered', 'across', 'floor'],\n",
              " ['brush', 'scattered', 'across', 'floor', 'shelf'],\n",
              " ['scattered', 'across', 'floor', 'shelf', 'toppled'],\n",
              " ['across', 'floor', 'shelf', 'toppled', 'sideways'],\n",
              " ['world', 'created', 'two', 'u', 'universe'],\n",
              " ['created', 'two', 'u', 'universe', 'supposed'],\n",
              " ['two', 'u', 'universe', 'supposed', 'inspire'],\n",
              " ['u', 'universe', 'supposed', 'inspire', 'future'],\n",
              " ['universe', 'supposed', 'inspire', 'future', 'stained'],\n",
              " ['supposed', 'inspire', 'future', 'stained', 'covered'],\n",
              " ['inspire', 'future', 'stained', 'covered', 'blood'],\n",
              " ['future', 'stained', 'covered', 'blood', 'paint'],\n",
              " ['stained', 'covered', 'blood', 'paint', 'split'],\n",
              " ['covered', 'blood', 'paint', 'split', 'knife'],\n",
              " ['hope', 'given', 'planet', 'lay', 'destroyed'],\n",
              " ['given', 'planet', 'lay', 'destroyed', 'middle'],\n",
              " ['planet', 'lay', 'destroyed', 'middle', 'room'],\n",
              " ['lay', 'destroyed', 'middle', 'room', 'broken'],\n",
              " ['destroyed', 'middle', 'room', 'broken', 'body'],\n",
              " ['glanced', 'eye', 'studying', 'faint', 'hint'],\n",
              " ['eye', 'studying', 'faint', 'hint', 'recognition'],\n",
              " ['studying', 'faint', 'hint', 'recognition', 'dread'],\n",
              " ['faint', 'hint', 'recognition', 'dread', 'mouth'],\n",
              " ['hint', 'recognition', 'dread', 'mouth', 'gagged'],\n",
              " ['recognition', 'dread', 'mouth', 'gagged', 'broken'],\n",
              " ['could', 'hear', 'whimper', 'softly', 'occasionally'],\n",
              " ['hear', 'whimper', 'softly', 'occasionally', 'sleep'],\n",
              " ['whimper', 'softly', 'occasionally', 'sleep', 'stood'],\n",
              " ['softly', 'occasionally', 'sleep', 'stood', 'watch'],\n",
              " ['spilled', 'paint', 'surrounded', 'mixed', 'single'],\n",
              " ['paint', 'surrounded', 'mixed', 'single', 'grotesque'],\n",
              " ['surrounded', 'mixed', 'single', 'grotesque', 'shade'],\n",
              " ['mixed', 'single', 'grotesque', 'shade', 'red'],\n",
              " ['single', 'grotesque', 'shade', 'red', 'blue'],\n",
              " ['grotesque', 'shade', 'red', 'blue', 'yellow'],\n",
              " ['shade', 'red', 'blue', 'yellow', 'white'],\n",
              " ['red', 'blue', 'yellow', 'white', 'every'],\n",
              " ['blue', 'yellow', 'white', 'every', 'color'],\n",
              " ['yellow', 'white', 'every', 'color', 'previously'],\n",
              " ['white', 'every', 'color', 'previously', 'organized'],\n",
              " ['every', 'color', 'previously', 'organized', 'shelf'],\n",
              " ['color', 'previously', 'organized', 'shelf', 'beside'],\n",
              " ['previously', 'organized', 'shelf', 'beside', 'door'],\n",
              " ['stared', 'moment', 'waiting', 'apology', 'eye'],\n",
              " ['moment', 'waiting', 'apology', 'eye', 'searched'],\n",
              " ['waiting', 'apology', 'eye', 'searched', 'perfection'],\n",
              " ['apology', 'eye', 'searched', 'perfection', 'hope'],\n",
              " ['eye', 'searched', 'perfection', 'hope', 'seen'],\n",
              " ['searched', 'perfection', 'hope', 'seen', 'long'],\n",
              " ['flawless', 'thing', 'could', 'save', 'world'],\n",
              " ['thing', 'could', 'save', 'world', 'pornographic'],\n",
              " ['could', 'save', 'world', 'pornographic', 'filthy'],\n",
              " ['save', 'world', 'pornographic', 'filthy', 'wreck'],\n",
              " ['world', 'pornographic', 'filthy', 'wreck', 'become'],\n",
              " ['lay', 'ground', 'eye', 'screaming', 'help'],\n",
              " ['ground', 'eye', 'screaming', 'help', 'could'],\n",
              " ['eye', 'screaming', 'help', 'could', 'see'],\n",
              " ['screaming', 'help', 'could', 'see', 'failure'],\n",
              " ['help', 'could', 'see', 'failure', 'dependence'],\n",
              " ['mirrored', 'figure', 'shifted', 'far', 'corner'],\n",
              " ['figure', 'shifted', 'far', 'corner', 'room'],\n",
              " ['shifted', 'far', 'corner', 'room', 'back'],\n",
              " ['turned', 'around', 'quietly', 'shut', 'door'],\n",
              " ['around', 'quietly', 'shut', 'door', 'began'],\n",
              " ['quietly', 'shut', 'door', 'began', 'back'],\n",
              " ['shut', 'door', 'began', 'back', 'path'],\n",
              " ['door', 'began', 'back', 'path', 'become'],\n",
              " ['began', 'back', 'path', 'become', 'familiar'],\n",
              " ['back', 'path', 'become', 'familiar', 'traveling'],\n",
              " ['alternate', 'ending', 'violent', 'exciting', 'one'],\n",
              " ['ending', 'violent', 'exciting', 'one', 'http'],\n",
              " ['violent', 'exciting', 'one', 'http', 'www'],\n",
              " ['com', 'r', 'writingprompts', 'comment', '2jws0e'],\n",
              " ['r', 'writingprompts', 'comment', '2jws0e', 'wpserial'],\n",
              " ['writingprompts', 'comment', '2jws0e', 'wpserial', 'killer'],\n",
              " ['comment', '2jws0e', 'wpserial', 'killer', 'monitoring'],\n",
              " ['2jws0e', 'wpserial', 'killer', 'monitoring', 'next'],\n",
              " ['wpserial', 'killer', 'monitoring', 'next', 'clg1ghi'],\n",
              " ['killer', 'monitoring', 'next', 'clg1ghi', 'le'],\n",
              " ['monitoring', 'next', 'clg1ghi', 'le', 'ambiguous'],\n",
              " ['next', 'clg1ghi', 'le', 'ambiguous', 'ending'],\n",
              " ['com', 'r', 'writingprompts', 'comment', '2jws0e'],\n",
              " ['r', 'writingprompts', 'comment', '2jws0e', 'wpserial'],\n",
              " ['writingprompts', 'comment', '2jws0e', 'wpserial', 'killer'],\n",
              " ['comment', '2jws0e', 'wpserial', 'killer', 'monitoring'],\n",
              " ['2jws0e', 'wpserial', 'killer', 'monitoring', 'next'],\n",
              " ['wpserial', 'killer', 'monitoring', 'next', 'clfxmdp'],\n",
              " ['killer', 'monitoring', 'next', 'clfxmdp', 'enjoy'],\n",
              " ['monitoring', 'next', 'clfxmdp', 'enjoy', 'writing'],\n",
              " ['next', 'clfxmdp', 'enjoy', 'writing', 'style'],\n",
              " ['clfxmdp', 'enjoy', 'writing', 'style', 'feel'],\n",
              " ['enjoy', 'writing', 'style', 'feel', 'free'],\n",
              " ['writing', 'style', 'feel', 'free', 'check'],\n",
              " ['style', 'feel', 'free', 'check', 'short'],\n",
              " ['feel', 'free', 'check', 'short', 'story'],\n",
              " ['free', 'check', 'short', 'story', 'new'],\n",
              " ['check', 'short', 'story', 'new', 'subreddit'],\n",
              " ['short', 'story', 'new', 'subreddit', 'http'],\n",
              " ['story', 'new', 'subreddit', 'http', 'www'],\n",
              " ['com', 'r', 'chokingvictimwrites', 'website', 'http'],\n",
              " ['r', 'chokingvictimwrites', 'website', 'http', 'wordsontheinternet'],\n",
              " ['march', '17', '15', 'disposed', 'without'],\n",
              " ['17', '15', 'disposed', 'without', 'complication'],\n",
              " ['seems', 'police', 'begun', 'piece', 'together'],\n",
              " ['police', 'begun', 'piece', 'together', 'connection'],\n",
              " ['begun', 'piece', 'together', 'connection', '1'],\n",
              " ['piece', 'together', 'connection', '1', '2'],\n",
              " ['together', 'connection', '1', '2', '4'],\n",
              " ['monitor', 'investigation', 'reevaluate', 'later', 'date'],\n",
              " ['longer', 'hear', '15', 'scream', 'clearly'],\n",
              " ['hear', '15', 'scream', 'clearly', 'mind'],\n",
              " ['maybe', 'around', '7', 'worked', 'seemed'],\n",
              " ['around', '7', 'worked', 'seemed', 'like'],\n",
              " ['7', 'worked', 'seemed', 'like', 'spot'],\n",
              " ['worked', 'seemed', 'like', 'spot', 'good'],\n",
              " ['seemed', 'like', 'spot', 'good', 'potential'],\n",
              " ['must', 'especially', 'careful', 'police', 'making'],\n",
              " ['especially', 'careful', 'police', 'making', 'progress'],\n",
              " ['call', 'check', 'around', '11', 'parent'],\n",
              " ['check', 'around', '11', 'parent', 'neighborhood'],\n",
              " ['around', '11', 'parent', 'neighborhood', 'tomorrow'],\n",
              " ['april', '9', 'one', 'target', 'maximum'],\n",
              " ['9', 'one', 'target', 'maximum', 'potential'],\n",
              " ['mid', 'thirty', 'average', 'build', 'brunette'],\n",
              " ['never', 'company', 'association', 'immediate', 'neighbor'],\n",
              " ['spends', 'hour', 'watering', 'hydrangea', 'garden'],\n",
              " ['must', 'continue', 'reconnaissance', 'ensure', 'surprise'],\n",
              " ['april', '17', 'confirmed', 'target', 'contact'],\n",
              " ['17', 'confirmed', 'target', 'contact', '11'],\n",
              " ['confirmed', 'target', 'contact', '11', 'parent'],\n",
              " ['target', 'contact', '11', 'parent', 'good'],\n",
              " ['police', 'figured', '4', 'worked', 'place'],\n",
              " ['figured', '4', 'worked', 'place', '9'],\n",
              " ['april', '30', 'living', 'situation', 'optimal'],\n",
              " ['ever', 'leaf', 'house', 'go', 'work'],\n",
              " ['leaf', 'house', 'go', 'work', 'grocery'],\n",
              " ['house', 'go', 'work', 'grocery', 'store'],\n",
              " ['go', 'work', 'grocery', 'store', 'library'],\n",
              " ['may', '14', 'two', 'week', 'schedule'],\n",
              " ['14', 'two', 'week', 'schedule', 'complete'],\n",
              " ['barely', 'acknowledges', 'employee', 'either', 'grocery'],\n",
              " ['acknowledges', 'employee', 'either', 'grocery', 'store'],\n",
              " ['employee', 'either', 'grocery', 'store', 'library'],\n",
              " ['may', '16', 'police', 'found', '9'],\n",
              " ['16', 'police', 'found', '9', 'body'],\n",
              " ['may', '22', 'predict', 'target', 'every'],\n",
              " ['22', 'predict', 'target', 'every', 'move'],\n",
              " ['predict', 'target', 'every', 'move', 'reconnaissance'],\n",
              " ['target', 'every', 'move', 'reconnaissance', 'complete'],\n",
              " ['time', 'perform', 'extensive', 'background', 'check'],\n",
              " ['perform', 'extensive', 'background', 'check', 'make'],\n",
              " ['extensive', 'background', 'check', 'make', 'sure'],\n",
              " ['background', 'check', 'make', 'sure', 'random'],\n",
              " ['check', 'make', 'sure', 'random', 'link'],\n",
              " ['make', 'sure', 'random', 'link', 'police'],\n",
              " ['sure', 'random', 'link', 'police', 'find'],\n",
              " ['11', 'parent', 'spoke', 'police', 'neighborhood'],\n",
              " ['parent', 'spoke', 'police', 'neighborhood', 'crawling'],\n",
              " ['spoke', 'police', 'neighborhood', 'crawling', 'obstacle'],\n",
              " ['june', '6', 'police', 'concluded', '11'],\n",
              " ['6', 'police', 'concluded', '11', 'dead'],\n",
              " ['police', 'concluded', '11', 'dead', 'end'],\n",
              " ['time', 'tomorrow', 'target', 'officially', '16'],\n",
              " ['can', 'not', 'resist', 'urge', 'attend'],\n",
              " ['left', 'attend', '9', 'funeral', 'matter'],\n",
              " ['attend', '9', 'funeral', 'matter', 'hour'],\n",
              " ['car', 'left', 'driveway', 'hydrangea', 'unwatered'],\n",
              " ['left', 'driveway', 'hydrangea', 'unwatered', 'garden'],\n",
              " ['driveway', 'hydrangea', 'unwatered', 'garden', 'front'],\n",
              " ['hydrangea', 'unwatered', 'garden', 'front', 'door'],\n",
              " ['unwatered', 'garden', 'front', 'door', 'locked'],\n",
              " ['june', '11', 'still', 'sign', '16'],\n",
              " ['two', 'month', 'flawless', 'consistency', 'broken'],\n",
              " ['day', '9', 'funeral', '16', 'disappears'],\n",
              " ['june', '21', 'one', 'miss', '16'],\n",
              " ['16', 'june', '25', 'simply', 'find'],\n",
              " ['june', '25', 'simply', 'find', 'another'],\n",
              " ['25', 'simply', 'find', 'another', 'target'],\n",
              " ['simply', 'find', 'another', 'target', 'forget'],\n",
              " ['find', 'another', 'target', 'forget', '16'],\n",
              " ['police', 'given', 'case', '9', 'funeral'],\n",
              " ['chance', 'caught', 'unless', 'mistake', 'made'],\n",
              " ['trying', 'find', '16', 'would', 'mistake'],\n",
              " ['june', '29', 'saw', 'movement', 'within'],\n",
              " ['29', 'saw', 'movement', 'within', '16'],\n",
              " ['saw', 'movement', 'within', '16', 'house'],\n",
              " ['movement', 'within', '16', 'house', 'today'],\n",
              " ['really', 'think', 'going', 'work', 'grady'],\n",
              " ['think', 'going', 'work', 'grady', 'little'],\n",
              " ['going', 'work', 'grady', 'little', 'faith'],\n",
              " ['work', 'grady', 'little', 'faith', 'holt'],\n",
              " ['moved', 'three', 'week', 'ago', 'nothing'],\n",
              " ['three', 'week', 'ago', 'nothing', 'happened'],\n",
              " ['week', 'ago', 'nothing', 'happened', 'yet'],\n",
              " ['would', 'u', 'coming', 'change', 'bet'],\n",
              " ['u', 'coming', 'change', 'bet', 'twenty'],\n",
              " ['coming', 'change', 'bet', 'twenty', 'buck'],\n",
              " ['change', 'bet', 'twenty', 'buck', 'watching'],\n",
              " ['bet', 'twenty', 'buck', 'watching', 'house'],\n",
              " ['twenty', 'buck', 'watching', 'house', 'right'],\n",
              " ['gon', 'na', 'suddenly', 'door', 'slid'],\n",
              " ['na', 'suddenly', 'door', 'slid', 'open'],\n",
              " ['suddenly', 'door', 'slid', 'open', 'squeak'],\n",
              " ['officer', 'sprang', 'foot', 'pistol', 'trained'],\n",
              " ['sprang', 'foot', 'pistol', 'trained', 'squarely'],\n",
              " ['foot', 'pistol', 'trained', 'squarely', 'intruder'],\n",
              " ['pistol', 'trained', 'squarely', 'intruder', 'head'],\n",
              " ['every', 'since', 'remember', 'number', 'head'],\n",
              " ['everyone', 'number', 'three', 'boy', 'pushed'],\n",
              " ['number', 'three', 'boy', 'pushed', 'slide'],\n",
              " ['three', 'boy', 'pushed', 'slide', 'seven'],\n",
              " ['boy', 'pushed', 'slide', 'seven', 'murderer'],\n",
              " ['pushed', 'slide', 'seven', 'murderer', 'tv'],\n",
              " ['slide', 'seven', 'murderer', 'tv', 'four'],\n",
              " ['seven', 'murderer', 'tv', 'four', 'mother'],\n",
              " ['murderer', 'tv', 'four', 'mother', 'etc'],\n",
              " ['never', 'met', 'ten', 'nine', 'eight'],\n",
              " ['met', 'ten', 'nine', 'eight', 'country'],\n",
              " ['perfect', 'kind', 'lovely', 'everything', 'ever'],\n",
              " ['kind', 'lovely', 'everything', 'ever', 'wanted'],\n",
              " ['lovely', 'everything', 'ever', 'wanted', 'life'],\n",
              " ['ten', 'everything', 'star', 'athlete', 'head'],\n",
              " ['everything', 'star', 'athlete', 'head', 'debate'],\n",
              " ['star', 'athlete', 'head', 'debate', 'team'],\n",
              " ['athlete', 'head', 'debate', 'team', 'model'],\n",
              " ['head', 'debate', 'team', 'model', 'student'],\n",
              " ['debate', 'team', 'model', 'student', 'loving'],\n",
              " ['team', 'model', 'student', 'loving', 'son'],\n",
              " ['everyone', 'knew', 'guy', 'wanted', 'girl'],\n",
              " ['knew', 'guy', 'wanted', 'girl', 'wanted'],\n",
              " ['wary', 'boy', 'seeming', 'deity', 'perfection'],\n",
              " ['boy', 'seeming', 'deity', 'perfection', 'would'],\n",
              " ['seeming', 'deity', 'perfection', 'would', 'ever'],\n",
              " ['deity', 'perfection', 'would', 'ever', 'make'],\n",
              " ['perfection', 'would', 'ever', 'make', 'like'],\n",
              " ['would', 'ever', 'make', 'like', 'terrified'],\n",
              " ['ever', 'make', 'like', 'terrified', 'moment'],\n",
              " ['make', 'like', 'terrified', 'moment', 'said'],\n",
              " ['like', 'terrified', 'moment', 'said', 'hello'],\n",
              " ['hey', 'alex', 'help', 'notice', 'always'],\n",
              " ['alex', 'help', 'notice', 'always', 'seem'],\n",
              " ['help', 'notice', 'always', 'seem', 'walk'],\n",
              " ['notice', 'always', 'seem', 'walk', 'away'],\n",
              " ['always', 'seem', 'walk', 'away', 'whenever'],\n",
              " ['seem', 'walk', 'away', 'whenever', 'room'],\n",
              " ['walk', 'away', 'whenever', 'room', 'upset'],\n",
              " ['away', 'whenever', 'room', 'upset', 'something'],\n",
              " ['whenever', 'room', 'upset', 'something', 'sorry'],\n",
              " ['room', 'upset', 'something', 'sorry', 'lady'],\n",
              " ['upset', 'something', 'sorry', 'lady', 'beautiful'],\n",
              " ['something', 'sorry', 'lady', 'beautiful', 'never'],\n",
              " ['sorry', 'lady', 'beautiful', 'never', 'uncomfortable'],\n",
              " ['blinked', 'wide', 'eyed', 'fear', 'eye'],\n",
              " ['wide', 'eyed', 'fear', 'eye', 'staring'],\n",
              " ['eyed', 'fear', 'eye', 'staring', 'perfectly'],\n",
              " ['fear', 'eye', 'staring', 'perfectly', 'sculpted'],\n",
              " ['eye', 'staring', 'perfectly', 'sculpted', 'outstretched'],\n",
              " ['staring', 'perfectly', 'sculpted', 'outstretched', 'hand'],\n",
              " ['wrong', 'hey', 'gon', 'na', 'bite'],\n",
              " ['focused', 'perfect', 'lip', 'way', 'white'],\n",
              " ['perfect', 'lip', 'way', 'white', 'teeth'],\n",
              " ['lip', 'way', 'white', 'teeth', 'broadened'],\n",
              " ['way', 'white', 'teeth', 'broadened', 'lovely'],\n",
              " ['white', 'teeth', 'broadened', 'lovely', 'grin'],\n",
              " ['boy', 'many', 'fell', 'love', 'buying'],\n",
              " ['different', 'one', 'weird', 'chick', 'analyzes'],\n",
              " ['one', 'weird', 'chick', 'analyzes', 'everything'],\n",
              " ['weird', 'chick', 'analyzes', 'everything', 'emotion'],\n",
              " ['chick', 'analyzes', 'everything', 'emotion', 'everyone'],\n",
              " ['analyzes', 'everything', 'emotion', 'everyone', 'stayed'],\n",
              " ['everything', 'emotion', 'everyone', 'stayed', 'away'],\n",
              " ['flora', 'rasped', 'voice', 'low', 'uncertain'],\n",
              " ['smile', 'grew', 'wider', 'hand', 'enveloped'],\n",
              " ['grew', 'wider', 'hand', 'enveloped', 'mine'],\n",
              " ['shock', 'jolted', 'arm', 'unexpected', 'yanked'],\n",
              " ['jolted', 'arm', 'unexpected', 'yanked', 'arm'],\n",
              " ['arm', 'unexpected', 'yanked', 'arm', 'back'],\n",
              " ['unexpected', 'yanked', 'arm', 'back', 'soon'],\n",
              " ['yanked', 'arm', 'back', 'soon', 'skin'],\n",
              " ['arm', 'back', 'soon', 'skin', 'met'],\n",
              " ['face', 'developed', 'seemed', 'like', 'cheshire'],\n",
              " ['developed', 'seemed', 'like', 'cheshire', 'grin'],\n",
              " ['saw', 'glint', 'eye', 'felt', 'true'],\n",
              " ['glint', 'eye', 'felt', 'true', 'terror'],\n",
              " ['eye', 'felt', 'true', 'terror', 'first'],\n",
              " ['felt', 'true', 'terror', 'first', 'time'],\n",
              " ['true', 'terror', 'first', 'time', 'life'],\n",
              " ['flora', 'repeated', 'test', 'word', 'tongue'],\n",
              " ['repeated', 'test', 'word', 'tongue', 'hoping'],\n",
              " ['test', 'word', 'tongue', 'hoping', 'find'],\n",
              " ['word', 'tongue', 'hoping', 'find', 'soon'],\n",
              " ['licked', 'lip', 'took', 'step', 'forward'],\n",
              " ['decided', 'go', '1', '15', 'scale'],\n",
              " ['go', '1', '15', 'scale', 'instead'],\n",
              " ['1', '15', 'scale', 'instead', '1'],\n",
              " ['15', 'scale', 'instead', '1', '10'],\n",
              " ['scale', 'instead', '1', '10', 'hope'],\n",
              " ['instead', '1', '10', 'hope', 'big'],\n",
              " ['1', '10', 'hope', 'big', 'deal'],\n",
              " ['something', 'hang', 'air', 'head', 'sort'],\n",
              " ['way', 'someone', 'stand', 'look', 'face'],\n",
              " ['number', 'far', 'tell', 'represent', 'dangerous'],\n",
              " ['far', 'tell', 'represent', 'dangerous', 'someone'],\n",
              " ['tell', 'represent', 'dangerous', 'someone', 'potential'],\n",
              " ['represent', 'dangerous', 'someone', 'potential', 'scale'],\n",
              " ['dangerous', 'someone', 'potential', 'scale', 'one'],\n",
              " ['someone', 'potential', 'scale', 'one', 'fifteen'],\n",
              " ['number', 'change', 'time', 'grandfather', 'vietnam'],\n",
              " ['change', 'time', 'grandfather', 'vietnam', 'veteran'],\n",
              " ['time', 'grandfather', 'vietnam', 'veteran', 'younger'],\n",
              " ['grandfather', 'vietnam', 'veteran', 'younger', 'number'],\n",
              " ['vietnam', 'veteran', 'younger', 'number', 'seven'],\n",
              " ['veteran', 'younger', 'number', 'seven', 'every'],\n",
              " ['younger', 'number', 'seven', 'every', 'year'],\n",
              " ['number', 'seven', 'every', 'year', 'drop'],\n",
              " ['seven', 'every', 'year', 'drop', 'point'],\n",
              " ['kid', 'tend', 'two', 'three', 'one'],\n",
              " ['tend', 'two', 'three', 'one', 'ever'],\n",
              " ['two', 'three', 'one', 'ever', 'seen'],\n",
              " ['three', 'one', 'ever', 'seen', 'quadriplegic'],\n",
              " ['big', 'number', 'tend', 'politician', 'world'],\n",
              " ['number', 'tend', 'politician', 'world', 'leader'],\n",
              " ['serial', 'killer', 'usually', 'hit', 'around'],\n",
              " ['killer', 'usually', 'hit', 'around', 'ten'],\n",
              " ['leader', 'general', 'involved', 'war', 'hit'],\n",
              " ['general', 'involved', 'war', 'hit', 'around'],\n",
              " ['involved', 'war', 'hit', 'around', 'thirteen'],\n",
              " ['fifteen', 'ever', 'seen', 'old', 'video'],\n",
              " ['ever', 'seen', 'old', 'video', 'least'],\n",
              " ['seen', 'old', 'video', 'least', 'today'],\n",
              " ['old', 'video', 'least', 'today', 'hitler'],\n",
              " ['video', 'least', 'today', 'hitler', 'mao'],\n",
              " ['least', 'today', 'hitler', 'mao', 'stalin'],\n",
              " ['today', 'hitler', 'mao', 'stalin', 'people'],\n",
              " ['hitler', 'mao', 'stalin', 'people', 'immediately'],\n",
              " ['mao', 'stalin', 'people', 'immediately', 'surround'],\n",
              " ['sort', 'people', 'responsible', 'mass', 'genocide'],\n",
              " ['people', 'responsible', 'mass', 'genocide', 'similar'],\n",
              " ['responsible', 'mass', 'genocide', 'similar', 'atrocity'],\n",
              " ['today', 'new', 'kid', 'came', 'school'],\n",
              " ['number', 'low', 'three', 'looked', 'smiled'],\n",
              " ['suddenly', 'felt', 'oppressive', 'fear', 'saw'],\n",
              " ['felt', 'oppressive', 'fear', 'saw', 'number'],\n",
              " ['oppressive', 'fear', 'saw', 'number', 'shoot'],\n",
              " ['fear', 'saw', 'number', 'shoot', 'fifteen'],\n",
              " ['passed', 'school', 'nurse', 'sent', 'home'],\n",
              " ['loading', 'dad', 'gun', 'let', 'live'],\n",
              " ['chance', 'kill', 'hitler', 'anything', 'wrong'],\n",
              " ['kill', 'hitler', 'anything', 'wrong', 'would'],\n",
              " ['hitler', 'anything', 'wrong', 'would', 'made'],\n",
              " ['anything', 'wrong', 'would', 'made', 'decision'],\n",
              " ['one', 'day', 'waiting', 'friend', 'park'],\n",
              " ['eating', 'chocolate', 'bar', 'apathetically', 'tossed'],\n",
              " ['chocolate', 'bar', 'apathetically', 'tossed', 'wrapper'],\n",
              " ['bar', 'apathetically', 'tossed', 'wrapper', 'aside'],\n",
              " ['naturally', 'ignored', 'lovely', 'young', 'man'],\n",
              " ['ignored', 'lovely', 'young', 'man', 'screamed'],\n",
              " ['fine', 'way', 'shall', 'litter', 'earth'],\n",
              " ['way', 'shall', 'litter', 'earth', 'shall'],\n",
              " ['shall', 'litter', 'earth', 'shall', 'litter'],\n",
              " ['litter', 'earth', 'shall', 'litter', 'mind'],\n",
              " ['promptly', 'left', 'confused', 'assumed', 'bit'],\n",
              " ['left', 'confused', 'assumed', 'bit', 'old'],\n",
              " ['confused', 'assumed', 'bit', 'old', 'bat'],\n",
              " ['wrapper', 'blew', 'away', 'friend', 'turned'],\n",
              " ['played', 'football', 'always', 'fairly', 'horrendous'],\n",
              " ['reason', 'first', 'thing', 'thought', 'old'],\n",
              " ['first', 'thing', 'thought', 'old', 'lady'],\n",
              " ['realised', 'fairly', 'obnoxious', 'act', 'way'],\n",
              " ['fairly', 'obnoxious', 'act', 'way', 'someone'],\n",
              " ['ah', 'well', 'probably', 'never', 'see'],\n",
              " ['well', 'probably', 'never', 'see', 'anyway'],\n",
              " ['went', 'downstairs', 'greeted', 'mother', 'bit'],\n",
              " ['downstairs', 'greeted', 'mother', 'bit', 'struggling'],\n",
              " ['greeted', 'mother', 'bit', 'struggling', 'single'],\n",
              " ['mother', 'bit', 'struggling', 'single', 'guess'],\n",
              " ['bit', 'struggling', 'single', 'guess', 'obnoxious'],\n",
              " ['struggling', 'single', 'guess', 'obnoxious', 'teenage'],\n",
              " ['single', 'guess', 'obnoxious', 'teenage', 'brat'],\n",
              " ['guess', 'obnoxious', 'teenage', 'brat', 'help'],\n",
              " ['obnoxious', 'teenage', 'brat', 'help', 'much'],\n",
              " ['two', 'hovering', 'head', 'cyan', 'colour'],\n",
              " ['intuitively', 'however', 'felt', 'oddly', 'natural'],\n",
              " ['went', 'school', 'day', 'noticed', 'everybody'],\n",
              " ['school', 'day', 'noticed', 'everybody', 'number'],\n",
              " ['notably', 'friend', 'tim', 'brown', 'belt'],\n",
              " ['friend', 'tim', 'brown', 'belt', 'karate'],\n",
              " ['tim', 'brown', 'belt', 'karate', 'blue'],\n",
              " ['brown', 'belt', 'karate', 'blue', 'four'],\n",
              " ['belt', 'karate', 'blue', 'four', 'loud'],\n",
              " ['karate', 'blue', 'four', 'loud', 'ex'],\n",
              " ['blue', 'four', 'loud', 'ex', 'military'],\n",
              " ['four', 'loud', 'ex', 'military', 'teacher'],\n",
              " ['loud', 'ex', 'military', 'teacher', 'enjoyed'],\n",
              " ['ex', 'military', 'teacher', 'enjoyed', 'startling'],\n",
              " ['military', 'teacher', 'enjoyed', 'startling', 'sleeping'],\n",
              " ['teacher', 'enjoyed', 'startling', 'sleeping', 'student'],\n",
              " ['enjoyed', 'startling', 'sleeping', 'student', 'yellow'],\n",
              " ['startling', 'sleeping', 'student', 'yellow', 'six'],\n",
              " ['student', 'one', 'two', 'shade', 'green'],\n",
              " ['could', 'never', 'see', 'number', 'head'],\n",
              " ['never', 'see', 'number', 'head', 'reflection'],\n",
              " ['see', 'number', 'head', 'reflection', 'anything'],\n",
              " ['number', 'head', 'reflection', 'anything', 'like'],\n",
              " ['head', 'reflection', 'anything', 'like', 'much'],\n",
              " ['reflection', 'anything', 'like', 'much', 'frustration'],\n",
              " ['watched', 'tv', 'noticed', 'powerful', 'people'],\n",
              " ['tv', 'noticed', 'powerful', 'people', 'tended'],\n",
              " ['noticed', 'powerful', 'people', 'tended', 'quite'],\n",
              " ['powerful', 'people', 'tended', 'quite', 'high'],\n",
              " ['people', 'tended', 'quite', 'high', 'number'],\n",
              " ['anchor', 'five', 'prime', 'minister', 'nine'],\n",
              " ['five', 'prime', 'minister', 'nine', 'footage'],\n",
              " ['prime', 'minister', 'nine', 'footage', 'army'],\n",
              " ['minister', 'nine', 'footage', 'army', 'parade'],\n",
              " ['nine', 'footage', 'army', 'parade', 'seemed'],\n",
              " ['footage', 'army', 'parade', 'seemed', 'show'],\n",
              " ['army', 'parade', 'seemed', 'show', 'range'],\n",
              " ['parade', 'seemed', 'show', 'range', 'six'],\n",
              " ['seemed', 'show', 'range', 'six', 'eight'],\n",
              " ['show', 'range', 'six', 'eight', 'vibrant'],\n",
              " ['range', 'six', 'eight', 'vibrant', 'red'],\n",
              " ['eventually', 'thought', 'week', 'concluded', 'number'],\n",
              " ['thought', 'week', 'concluded', 'number', 'corresponded'],\n",
              " ['week', 'concluded', 'number', 'corresponded', 'danger'],\n",
              " ['mean', 'potent', 'fight', 'mean', 'lot'],\n",
              " ['potent', 'fight', 'mean', 'lot', 'say'],\n",
              " ['fight', 'mean', 'lot', 'say', 'socially'],\n",
              " ['one', 'day', 'sitting', 'park', 'mother'],\n",
              " ['day', 'sitting', 'park', 'mother', 'friend'],\n",
              " ['sitting', 'park', 'mother', 'friend', 'day'],\n",
              " ['short', 'goatee', 'slicked', 'back', 'hair'],\n",
              " ['goatee', 'slicked', 'back', 'hair', 'eight'],\n",
              " ['mum', 'introduced', 'sean', 'new', 'boyfriend'],\n",
              " ['control', 'little', 'outburst', 'must', 'admit'],\n",
              " ['little', 'outburst', 'must', 'admit', 'panicked'],\n",
              " ['outburst', 'must', 'admit', 'panicked', 'bit'],\n",
              " ['eight', 'member', 'cabinet', 'soldier', 'serial'],\n",
              " ['member', 'cabinet', 'soldier', 'serial', 'murderer'],\n",
              " ['cabinet', 'soldier', 'serial', 'murderer', 'number'],\n",
              " ['soldier', 'serial', 'murderer', 'number', 'come'],\n",
              " ['serial', 'murderer', 'number', 'come', 'crimewatch'],\n",
              " ['murderer', 'number', 'come', 'crimewatch', 'well'],\n",
              " ['number', 'come', 'crimewatch', 'well', 'know'],\n",
              " ['going', 'around', 'point', 'leaned', 'kissed'],\n",
              " ['around', 'point', 'leaned', 'kissed', 'mum'],\n",
              " ['following', 'day', 'took', 'mum', 'aside'],\n",
              " ['day', 'took', 'mum', 'aside', 'repeatedly'],\n",
              " ['took', 'mum', 'aside', 'repeatedly', 'try'],\n",
              " ['mum', 'aside', 'repeatedly', 'try', 'convince'],\n",
              " ['aside', 'repeatedly', 'try', 'convince', 'get'],\n",
              " ['came', 'later', 'day', 'bringing', 'home'],\n",
              " ['later', 'day', 'bringing', 'home', 'shopping'],\n",
              " ['day', 'bringing', 'home', 'shopping', 'bought'],\n",
              " ['bringing', 'home', 'shopping', 'bought', 'chocolate'],\n",
              " ['home', 'shopping', 'bought', 'chocolate', 'bar'],\n",
              " ['could', 'tell', 'anxious', 'buy', 'little'],\n",
              " ['tell', 'anxious', 'buy', 'little', 'thing'],\n",
              " ['anxious', 'buy', 'little', 'thing', 'try'],\n",
              " ['buy', 'little', 'thing', 'try', 'win'],\n",
              " ['little', 'thing', 'try', 'win', 'affection'],\n",
              " ['mum', 'love', 'despite', 'red', 'flag'],\n",
              " ['love', 'despite', 'red', 'flag', 'honestly'],\n",
              " ['despite', 'red', 'flag', 'honestly', 'settling'],\n",
              " ['red', 'flag', 'honestly', 'settling', 'arrangement'],\n",
              " ['first', 'began', 'complaint', 'jam', 'cupboard'],\n",
              " ['began', 'complaint', 'jam', 'cupboard', 'said'],\n",
              " ['complaint', 'jam', 'cupboard', 'said', 'put'],\n",
              " ['jam', 'cupboard', 'said', 'put', 'clean'],\n",
              " ['cupboard', 'said', 'put', 'clean', 'fucking'],\n",
              " ['said', 'put', 'clean', 'fucking', 'cat'],\n",
              " ['put', 'clean', 'fucking', 'cat', 'shit'],\n",
              " ['eventually', 'offered', 'fully', 'support', 'mother'],\n",
              " ['really', 'sure', 'able', 'job', 'speak'],\n",
              " ['sure', 'able', 'job', 'speak', 'seemed'],\n",
              " ['able', 'job', 'speak', 'seemed', 'work'],\n",
              " ['mum', 'quit', 'job', 'supermarket', 'much'],\n",
              " ['quit', 'job', 'supermarket', 'much', 'free'],\n",
              " ['job', 'supermarket', 'much', 'free', 'time'],\n",
              " ['fucking', 'dish', 'make', 'doctor', 'appointment'],\n",
              " ['mum', 'number', 'dropped', 'two', 'one'],\n",
              " ['eventually', 'started', 'thing', 'hitting', 'around'],\n",
              " ['notice', 'course', 'although', 'started', 'pick'],\n",
              " ['course', 'although', 'started', 'pick', 'sign'],\n",
              " ['right', 'conscience', 'nowhere', 'could', 'go'],\n",
              " ['briefly', 'ran', 'bathroom', 'try', 'collect'],\n",
              " ['ran', 'bathroom', 'try', 'collect', 'figure'],\n",
              " ['bathroom', 'try', 'collect', 'figure', 'fuck'],\n",
              " ['try', 'collect', 'figure', 'fuck', 'supposed'],\n",
              " ['tear', 'saw', 'stained', 'bathroom', 'mirror'],\n",
              " ['black', 'night', 'floating', 'right', 'head'],\n",
              " ['briefly', 'lucid', 'state', 'called', 'ambulance'],\n",
              " ['picked', 'largest', 'shard', 'glass', 'looked'],\n",
              " ['sean', 'let', 'fucking', 'talk', 'edit'],\n",
              " ['let', 'fucking', 'talk', 'edit', 'thanks'],\n",
              " ['fucking', 'talk', 'edit', 'thanks', 'much'],\n",
              " ['talk', 'edit', 'thanks', 'much', 'comment'],\n",
              " ['edit', 'thanks', 'much', 'comment', 'upvotes'],\n",
              " ['thanks', 'much', 'comment', 'upvotes', 'especially'],\n",
              " ['much', 'comment', 'upvotes', 'especially', 'kind'],\n",
              " ['comment', 'upvotes', 'especially', 'kind', 'folk'],\n",
              " ['upvotes', 'especially', 'kind', 'folk', 'gave'],\n",
              " ['especially', 'kind', 'folk', 'gave', 'gold'],\n",
              " ['kind', 'folk', 'gave', 'gold', 'hoped'],\n",
              " ['folk', 'gave', 'gold', 'hoped', 'nicer'],\n",
              " ['gave', 'gold', 'hoped', 'nicer', 'reception'],\n",
              " ['also', 'listen', 'audio', 'version', 'http'],\n",
              " ['listen', 'audio', 'version', 'http', 'www'],\n",
              " ['com', 'r', 'writingprompts', 'comment', '2ns30z'],\n",
              " ['r', 'writingprompts', 'comment', '2ns30z', 'wp'],\n",
              " ['writingprompts', 'comment', '2ns30z', 'wp', 'teenager'],\n",
              " ['comment', '2ns30z', 'wp', 'teenager', 'ability'],\n",
              " ['2ns30z', 'wp', 'teenager', 'ability', 'measure'],\n",
              " ['wp', 'teenager', 'ability', 'measure', 'cmhw3df'],\n",
              " ['scary', 'really', 'quickly', 'people', 'go'],\n",
              " ['really', 'quickly', 'people', 'go', 'six'],\n",
              " ['quickly', 'people', 'go', 'six', 'even'],\n",
              " ['people', 'go', 'six', 'even', 'five'],\n",
              " ['go', 'six', 'even', 'five', 'ten'],\n",
              " ['people', 'realize', 'much', 'danger', 'surround'],\n",
              " ['realize', 'much', 'danger', 'surround', 'every'],\n",
              " ['much', 'danger', 'surround', 'every', 'day'],\n",
              " ['one', 'always', 'mind', 'something', 'else'],\n",
              " ['always', 'mind', 'something', 'else', 'entirely'],\n",
              " ['first', 'zero', 'ever', 'saw', 'one'],\n",
              " ['zero', 'ever', 'saw', 'one', 'watched'],\n",
              " ['ever', 'saw', 'one', 'watched', 'child'],\n",
              " ['saw', 'one', 'watched', 'child', 'born'],\n",
              " ['eventually', 'grew', 'little', 'one', 'two'],\n",
              " ['grew', 'little', 'one', 'two', 'course'],\n",
              " ['little', 'one', 'two', 'course', 'short'],\n",
              " ['one', 'two', 'course', 'short', 'wonderful'],\n",
              " ['two', 'course', 'short', 'wonderful', 'time'],\n",
              " ['course', 'short', 'wonderful', 'time', 'tiny'],\n",
              " ['short', 'wonderful', 'time', 'tiny', 'giggling'],\n",
              " ['wonderful', 'time', 'tiny', 'giggling', 'bundle'],\n",
              " ['time', 'tiny', 'giggling', 'bundle', 'zero'],\n",
              " ['drawn', 'moment', 'saw', 'dangling', 'leg'],\n",
              " ['moment', 'saw', 'dangling', 'leg', 'swing'],\n",
              " ['saw', 'dangling', 'leg', 'swing', 'playground'],\n",
              " ['old', 'could', '12', '13', 'maybe'],\n",
              " ['really', 'sure', 'number', 'meant', 'lowest'],\n",
              " ['sure', 'number', 'meant', 'lowest', 'ever'],\n",
              " ['number', 'meant', 'lowest', 'ever', 'seen'],\n",
              " ['meant', 'lowest', 'ever', 'seen', 'one'],\n",
              " ['lowest', 'ever', 'seen', 'one', 'knew'],\n",
              " ['ever', 'seen', 'one', 'knew', 'something'],\n",
              " ['seen', 'one', 'knew', 'something', 'special'],\n",
              " ['stared', 'across', 'playground', 'smiled', 'entire'],\n",
              " ['across', 'playground', 'smiled', 'entire', 'face'],\n",
              " ['playground', 'smiled', 'entire', 'face', 'beaming'],\n",
              " ['smiled', 'entire', 'face', 'beaming', 'zero'],\n",
              " ['say', 'helen', 'troy', 'face', 'launched'],\n",
              " ['helen', 'troy', 'face', 'launched', 'thousand'],\n",
              " ['troy', 'face', 'launched', 'thousand', 'ship'],\n",
              " ['well', 'sarah', 'could', 'make', 'come'],\n",
              " ['sarah', 'could', 'make', 'come', 'back'],\n",
              " ['could', 'make', 'come', 'back', 'home'],\n",
              " ['ever', 'face', 'could', 'end', 'war'],\n",
              " ['terrified', 'ruining', 'saying', 'much', 'word'],\n",
              " ['ruining', 'saying', 'much', 'word', 'wanted'],\n",
              " ['saying', 'much', 'word', 'wanted', 'say'],\n",
              " ['much', 'word', 'wanted', 'say', 'ran'],\n",
              " ['word', 'wanted', 'say', 'ran', 'lap'],\n",
              " ['wanted', 'say', 'ran', 'lap', 'head'],\n",
              " ['say', 'ran', 'lap', 'head', 'every'],\n",
              " ['ran', 'lap', 'head', 'every', 'time'],\n",
              " ['lap', 'head', 'every', 'time', 'saw'],\n",
              " ['hot', 'summer', 'night', 'made', 'drunken'],\n",
              " ['summer', 'night', 'made', 'drunken', 'mistake'],\n",
              " ['night', 'made', 'drunken', 'mistake', 'said'],\n",
              " ['made', 'drunken', 'mistake', 'said', 'anyway'],\n",
              " ['want', 'feel', 'felt', 'back', 'summer'],\n",
              " ['feel', 'felt', 'back', 'summer', 'night'],\n",
              " ['felt', 'back', 'summer', 'night', 'know'],\n",
              " ['back', 'summer', 'night', 'know', 'find'],\n",
              " ['summer', 'night', 'know', 'find', 'someone'],\n",
              " ['night', 'know', 'find', 'someone', 'share'],\n",
              " ['know', 'find', 'someone', 'share', 'moment'],\n",
              " ['find', 'someone', 'share', 'moment', 'happy'],\n",
              " ['someone', 'share', 'moment', 'happy', 'rest'],\n",
              " ['share', 'moment', 'happy', 'rest', 'life'],\n",
              " ['got', 'married', 'got', 'nice', 'job'],\n",
              " ['first', 'noticed', 'one', 'saturday', 'morning'],\n",
              " ['noticed', 'one', 'saturday', 'morning', 'laundry'],\n",
              " ['long', 'hour', 'work', 'week', 'feeling'],\n",
              " ['hour', 'work', 'week', 'feeling', 'well'],\n",
              " ['work', 'week', 'feeling', 'well', 'offered'],\n",
              " ['week', 'feeling', 'well', 'offered', 'laundry'],\n",
              " ['instead', 'clear', 'bright', 'zero', 'used'],\n",
              " ['clear', 'bright', 'zero', 'used', 'seeing'],\n",
              " ['bright', 'zero', 'used', 'seeing', 'flashing'],\n",
              " ['zero', 'used', 'seeing', 'flashing', 'faint'],\n",
              " ['used', 'seeing', 'flashing', 'faint', 'one'],\n",
              " ['stunned', 'first', 'managed', 'convince', 'nothing'],\n",
              " ['behaving', 'like', 'usual', 'self', 'gave'],\n",
              " ['like', 'usual', 'self', 'gave', 'dull'],\n",
              " ['usual', 'self', 'gave', 'dull', 'weak'],\n",
              " ['self', 'gave', 'dull', 'weak', 'three'],\n",
              " ['gave', 'dull', 'weak', 'three', 'like'],\n",
              " ['dull', 'weak', 'three', 'like', 'trying'],\n",
              " ['weak', 'three', 'like', 'trying', 'desperately'],\n",
              " ['three', 'like', 'trying', 'desperately', 'calm'],\n",
              " ['asked', 'something', 'wrong', 'mumbled', 'something'],\n",
              " ['something', 'wrong', 'mumbled', 'something', 'asshole'],\n",
              " ['wrong', 'mumbled', 'something', 'asshole', 'work'],\n",
              " ['wanted', 'dig', 'deeper', 'take', 'care'],\n",
              " ['dig', 'deeper', 'take', 'care', 'kid'],\n",
              " ['racked', 'brain', 'week', 'trying', 'find'],\n",
              " ['brain', 'week', 'trying', 'find', 'wrong'],\n",
              " ['something', 'done', 'get', 'fired', 'unthinkable'],\n",
              " ['done', 'get', 'fired', 'unthinkable', 'sick'],\n",
              " ['get', 'fired', 'unthinkable', 'sick', 'tell'],\n",
              " ['fired', 'unthinkable', 'sick', 'tell', 'thought'],\n",
              " ['unthinkable', 'sick', 'tell', 'thought', 'secret'],\n",
              " ['never', 'came', 'home', 'thursday', 'night'],\n",
              " ['bed', 'staring', 'ceiling', 'running', 'thought'],\n",
              " ['staring', 'ceiling', 'running', 'thought', 'mind'],\n",
              " ['going', 'get', 'call', 'police', 'hospital'],\n",
              " ['get', 'call', 'police', 'hospital', 'considered'],\n",
              " ['call', 'police', 'hospital', 'considered', 'calling'],\n",
              " ['police', 'hospital', 'considered', 'calling', 'see'],\n",
              " ['taking', 'shoe', 'walking', 'stair', 'something'],\n",
              " ['shoe', 'walking', 'stair', 'something', 'step'],\n",
              " ['walking', 'stair', 'something', 'step', 'putting'],\n",
              " ['stair', 'something', 'step', 'putting', 'hand'],\n",
              " ['something', 'step', 'putting', 'hand', 'door'],\n",
              " ['step', 'putting', 'hand', 'door', 'handle'],\n",
              " ['dead', 'silence', 'heard', 'take', 'deep'],\n",
              " ['silence', 'heard', 'take', 'deep', 'breath'],\n",
              " ['heard', 'take', 'deep', 'breath', 'side'],\n",
              " ['wore', 'work', 'clothes', 'bit', 'ruffled'],\n",
              " ['work', 'clothes', 'bit', 'ruffled', 'wrinkly'],\n",
              " ['looked', 'expression', 'never', 'seen', 'wear'],\n",
              " ['drew', 'another', 'breath', 'short', 'shallow'],\n",
              " ['another', 'breath', 'short', 'shallow', 'lip'],\n",
              " ['breath', 'short', 'shallow', 'lip', 'slowly'],\n",
              " ['short', 'shallow', 'lip', 'slowly', 'parted'],\n",
              " ['voice', 'sounded', 'muffled', 'ear', 'like'],\n",
              " ['sounded', 'muffled', 'ear', 'like', 'coming'],\n",
              " ['muffled', 'ear', 'like', 'coming', 'far'],\n",
              " ['ear', 'like', 'coming', 'far', 'away'],\n",
              " ['standing', 'bedroom', 'second', 'floor', 'house'],\n",
              " ['room', 'across', 'hall', 'child', 'sleeping'],\n",
              " ['watched', 'standing', 'looked', 'back', 'face'],\n",
              " ['standing', 'looked', 'back', 'face', 'screaming'],\n",
              " ['looked', 'back', 'face', 'screaming', 'hot'],\n",
              " ['back', 'face', 'screaming', 'hot', 'burning'],\n",
              " ['face', 'screaming', 'hot', 'burning', 'ten'],\n",
              " ['pun', 'der', 'arrest', 'right', 'remain'],\n",
              " ['der', 'arrest', 'right', 'remain', 'silent'],\n",
              " ['arrest', 'right', 'remain', 'silent', 'told'],\n",
              " ['right', 'remain', 'silent', 'told', 'man'],\n",
              " ['remain', 'silent', 'told', 'man', 'handled'],\n",
              " ['silent', 'told', 'man', 'handled', 'back'],\n",
              " ['told', 'man', 'handled', 'back', 'police'],\n",
              " ['man', 'handled', 'back', 'police', 'cruiser'],\n",
              " ['everything', 'say', 'used', 'make', 'pun'],\n",
              " ['make', 'little', 'tweak', 'miranda', 'warning'],\n",
              " ['little', 'tweak', 'miranda', 'warning', 'ever'],\n",
              " ['tweak', 'miranda', 'warning', 'ever', 'since'],\n",
              " ['miranda', 'warning', 'ever', 'since', 'bad'],\n",
              " ['warning', 'ever', 'since', 'bad', 'joke'],\n",
              " ['ever', 'since', 'bad', 'joke', 'became'],\n",
              " ['since', 'bad', 'joke', 'became', 'physically'],\n",
              " ['bad', 'joke', 'became', 'physically', 'hurtful'],\n",
              " ['nothing', 'shouted', 'got', 'back', 'driver'],\n",
              " ['shouted', 'got', 'back', 'driver', 'seat'],\n",
              " ['always', 'ignore', 'warning', 'well', 'sound'],\n",
              " ['ignore', 'warning', 'well', 'sound', 'like'],\n",
              " ['warning', 'well', 'sound', 'like', 'nothing'],\n",
              " ['well', 'sound', 'like', 'nothing', 'lucky'],\n",
              " ['sound', 'like', 'nothing', 'lucky', 'gal'],\n",
              " ['like', 'nothing', 'lucky', 'gal', 'replied'],\n",
              " ['winced', 'rolled', 'eye', 'getting', 'started'],\n",
              " ['dad', 'joke', 'appetizer', 'meal', 'serve'],\n",
              " ['fumbled', 'around', 'seatbelt', 'making', 'exaggerated'],\n",
              " ['around', 'seatbelt', 'making', 'exaggerated', 'sound'],\n",
              " ['seatbelt', 'making', 'exaggerated', 'sound', 'frustration'],\n",
              " ['making', 'exaggerated', 'sound', 'frustration', 'stopped'],\n",
              " ['exaggerated', 'sound', 'frustration', 'stopped', 'complaining'],\n",
              " ['gripped', 'barrier', 'u', 'shook', 'violently'],\n",
              " ['barrier', 'u', 'shook', 'violently', 'gritting'],\n",
              " ['u', 'shook', 'violently', 'gritting', 'teeth'],\n",
              " ['high', 'speed', 'chase', 'never', 'work'],\n",
              " ['speed', 'chase', 'never', 'work', 'criminal'],\n",
              " ['wreck', 'amend', 'rolled', 'agony', 'struggling'],\n",
              " ['amend', 'rolled', 'agony', 'struggling', 'handcuff'],\n",
              " ['rolled', 'agony', 'struggling', 'handcuff', 'cover'],\n",
              " ['agony', 'struggling', 'handcuff', 'cover', 'ear'],\n",
              " ['end', 'crash', 'clipped', 'billboard', 'ominous'],\n",
              " ['crash', 'clipped', 'billboard', 'ominous', 'managed'],\n",
              " ['clipped', 'billboard', 'ominous', 'managed', 'open'],\n",
              " ['billboard', 'ominous', 'managed', 'open', 'eye'],\n",
              " ['ominous', 'managed', 'open', 'eye', 'screwed'],\n",
              " ['managed', 'open', 'eye', 'screwed', 'tight'],\n",
              " ['combing', 'area', 'wig', 'thief', 'high'],\n",
              " ['area', 'wig', 'thief', 'high', 'speed'],\n",
              " ['wig', 'thief', 'high', 'speed', 'pursuit'],\n",
              " ['thief', 'high', 'speed', 'pursuit', 'ended'],\n",
              " ['high', 'speed', 'pursuit', 'ended', 'victim'],\n",
              " ['speed', 'pursuit', 'ended', 'victim', 'custody'],\n",
              " ['time', 'undergo', 'mitosis', 'going', 'need'],\n",
              " ['undergo', 'mitosis', 'going', 'need', 'another'],\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cV8A89Wuy2aI"
      },
      "outputs": [],
      "source": [
        "# Make a single array of all words from ngrams\n",
        "split_sentences = []\n",
        "for arr in n_grams:\n",
        "    split_sentences += arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eJ-aDHK0y2aJ"
      },
      "outputs": [],
      "source": [
        "def create_vocabulary(sentence):\n",
        "    # Create empty dictionaries\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "\n",
        "    # Get array of set of words\n",
        "    word_set = list(set(sentence))\n",
        "\n",
        "    # Iterate over set of words and save them to dictionaries\n",
        "    for i in range(len(word_set)):\n",
        "        word_to_index[word_set[i]] = i\n",
        "        index_to_word[i] = word_set[i]\n",
        "\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "\n",
        "def input_target_generator(n_grams: list, word_to_index: dict):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Iterate over ngrams\n",
        "    for i in range(len(n_grams)):\n",
        "\n",
        "        # Separate input from ngram and target\n",
        "        input_part = n_grams[i][:-1]\n",
        "        target_word = n_grams[i][-1]\n",
        "\n",
        "        # Create and append input and target\n",
        "        _input = []\n",
        "        _target = []\n",
        "        for word in input_part:\n",
        "            _input.append(word_to_index[word])\n",
        "        _target.append(word_to_index[target_word])\n",
        "\n",
        "        # Convert to torch arrays\n",
        "        _input = torch.tensor(_input, dtype=torch.long)\n",
        "        _target = torch.tensor(_target, dtype=torch.long)\n",
        "\n",
        "        inputs.append(_input)\n",
        "        targets.append(_target)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "# Create vocabulary and 'reverse' vocabulary\n",
        "word_to_index, index_to_word = create_vocabulary(sentence=split_sentences)\n",
        "# Create arrays of inputs and targets\n",
        "inputs, targets = input_target_generator(n_grams=n_grams, word_to_index=word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yLdZFXFT23s",
        "outputId": "fc61e731-0848-430a-cafe-c20e0289ccc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('said', 1363), ('one', 1348), ('like', 1225), ('time', 1013), ('would', 912), ('know', 910), ('back', 909), ('could', 888), ('eye', 733), ('man', 681)]\n",
            "total words: 177772\n",
            "Frequencies of 10 most common word\n",
            "said: 0.76671241815359\n",
            "one: 0.7582746439259276\n",
            "like: 0.6890848952590959\n",
            "time: 0.569831019508134\n",
            "would: 0.5130166730418739\n",
            "know: 0.5118916364781855\n",
            "back: 0.5113291181963414\n",
            "could: 0.499516234277614\n",
            "eye: 0.4123259005917692\n",
            "man: 0.38307494993587293\n"
          ]
        }
      ],
      "source": [
        "# Join sentences in each Story then join all stories\n",
        "joined_sentences = [' '.join(sentences) for sentences in reddit_df['sentences']]\n",
        "all_stories = ' '.join(joined_sentences)\n",
        "\n",
        "# Tokenize the words\n",
        "words = nltk.word_tokenize(all_stories)\n",
        "\n",
        "# Count the occurrences of each word\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Top 10 most occuring words\n",
        "top_10_words = word_counts.most_common(10)\n",
        "print(top_10_words)\n",
        "\n",
        "# The total number of words\n",
        "total_words = sum(word_counts.values())\n",
        "print(f\"total words: {total_words}\")\n",
        "\n",
        "# Get the frequencies of each word\n",
        "word_frequencies = {}\n",
        "for word, count in word_counts.items():\n",
        "    word_frequencies[word] = count / total_words\n",
        "    \n",
        "# Top 10 word's frequencies\n",
        "sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "top_10_word_frequencies = sorted_word_frequencies[:10]\n",
        "print(\"Frequencies of 10 most common word\")\n",
        "for word, freq in top_10_word_frequencies:\n",
        "    print(f\"{word}: {freq *100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fXFc-u37y2aJ"
      },
      "outputs": [],
      "source": [
        "class StoryGenerator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_grams_size, n_layers=1):\n",
        "        super(StoryGenerator, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_grams_size = n_grams_size - 1\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size*self.n_grams_size,\n",
        "                            hidden_size=hidden_size, \n",
        "                            num_layers=n_layers, \n",
        "                            batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # Shape: (batch_size, seq_len, hidden_size)\n",
        "        input = self.embedding(input)\n",
        "        # Reshape to (batch_size, seq_len, hidden_size * (g_grams_size - 1))\n",
        "        input = input.view(input.size(0), -1, self.hidden_size * self.n_grams_size)\n",
        "        output, hidden = self.lstm(input, hidden)\n",
        "        # Shape: (batch_size, output_size)\n",
        "        output = self.linear(output[:, -1, :])\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        # LSTM requires tuple as output\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "djs8gf62y2aJ"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "TqJU3E2Py2aJ",
        "outputId": "f7b6c509-7043-4642-d71c-4a11a2895535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Iter 374. [Val Acc 0.6446363750156465%] [Train Acc 0.7155672382859766%, Loss 0.0651494488120079]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-17f70807867a>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_model\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-17f70807867a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, train_target, valid_data, valid_target, num_epochs, learning_rate, batch_size, checkpoint_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# compute updates for each parameters and make the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# clean up gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def get_accuracy(model, dataset):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create dataloader\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Disable gradient computation\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_target in data_loader:\n",
        "            hidden_h, hidden_c = model.init_hidden(batch_data.size(0))\n",
        "\n",
        "            output, (hidden_h, hidden_c) = model(batch_data, (hidden_h, hidden_c))\n",
        "            _, predicted = torch.max(output, dim=1)\n",
        "            correct += (predicted == batch_target.squeeze()).sum().item()\n",
        "            total += batch_target.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train(model, train_data, train_target, valid_data, valid_target, num_epochs, learning_rate, batch_size=32, checkpoint_path=None):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Convert the lists of tensors to a single tensor\n",
        "    train_data_tensor = torch.stack(train_data)\n",
        "    train_target_tensor = torch.stack(train_target)\n",
        "\n",
        "    valid_data_tensor = torch.stack(valid_data)\n",
        "    valid_target_tensor = torch.stack(valid_target)\n",
        "\n",
        "    # Create a TensorDataset and DataLoader for mini-batches\n",
        "    train_dataset = TensorDataset(train_data_tensor, train_target_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    valid_dataset = TensorDataset(valid_data_tensor, valid_target_tensor)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "    n = 0 # nums of iterations\n",
        "    for i in range(0, num_epochs):\n",
        "        hidden_h, hidden_c = model.init_hidden(batch_size)\n",
        "        for batch_num, (batch_train_data, batch_train_target) in enumerate(train_loader):  \n",
        "            # clean up gradient\n",
        "            optimizer.zero_grad()\n",
        "            # forward step\n",
        "            output, (hidden_h, hidden_c) = model(batch_train_data, (hidden_h, hidden_c))\n",
        "            # compute total loss\n",
        "            loss = criterion(output, batch_train_target.squeeze())\n",
        "            # detach hidden_h and hidden_c for next iteration\n",
        "            hidden_h = hidden_h.detach()\n",
        "            hidden_c = hidden_c.detach()\n",
        "            \n",
        "            # compute updates for each parameters and make the update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # clean up gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)\n",
        "            n += 1\n",
        "\n",
        "        train_accuracy = get_accuracy(model, train_dataset)\n",
        "        valid_accuracy = get_accuracy(model, valid_dataset)\n",
        "        train_cost = float(loss)/batch_size\n",
        "\n",
        "        iters_sub.append(n)\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(valid_accuracy)\n",
        "        print('Epoch {}. Iter {}. [Val Acc {}%] [Train Acc {}%, Loss {}]'.format(i+1, n, valid_accuracy * 100, train_accuracy * 100, train_cost))\n",
        "        if checkpoint_path is not None:\n",
        "            torch.save(model.state_dict(), checkpoint_path.format(i))\n",
        "          \n",
        "\n",
        "    # return iters, losses, iters_sub, train_accs, val_accs\n",
        "    plot(iters, losses, iters_sub, train_accs, val_accs)\n",
        "\n",
        "\n",
        "\n",
        "def plot(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "num_epochs = 15\n",
        "hidden_size = 128\n",
        "n_layers = 3\n",
        "learning_rate = 0.015\n",
        "# checkpoint_path = \"./weights/epoch-{}.pk\"\n",
        "\n",
        "vocab_length = len(word_to_index)\n",
        "story_model = StoryGenerator(input_size=vocab_length, hidden_size=hidden_size, output_size=vocab_length, n_layers=n_layers, n_grams_size=5)\n",
        "\n",
        "\n",
        "train(story_model,  x_train, y_train, x_valid, y_valid, num_epochs, learning_rate, batch_size=128, checkpoint_path=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h1RjODiy2aK",
        "outputId": "a362b8cb-e432-4d73-e6da-602599b72f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sliding across wet floor hit police 33rd witha never answered lifelong instead competent prayer man extra eternal fear table edge word fatal problem finally scope tee\n"
          ]
        }
      ],
      "source": [
        "def generate(model, index_to_word, prompt='sliding across wet floor hit police', length=20, n_grams_size=5):\n",
        "    model.eval()\n",
        "    batch_size = 1\n",
        "    hidden_h, hidden_c = story_model.init_hidden(batch_size)\n",
        "    n_grams_size -= 1\n",
        "    for prediction in range(length):\n",
        "        prompt_split = [word_to_index[w] for w in prompt.split()]\n",
        "        prompt_tensor = torch.tensor(prompt_split[-n_grams_size:]).unsqueeze(0)\n",
        "        # Get predictions\n",
        "        output, (hidden_h, hidden_c) = model(prompt_tensor, (hidden_h, hidden_c))\n",
        "        # Make distribution\n",
        "        distribution = output.data.view(-1).exp()\n",
        "        # Sample from distribution\n",
        "        sample = torch.multinomial(distribution, 1)[0].item()\n",
        "        # Search for word in 'reverse' dictionry\n",
        "        predicted_word = index_to_word[sample]\n",
        "        # Add word to prompt\n",
        "        prompt += \" \" + predicted_word\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Generate text based on input prompt\n",
        "print(generate(story_model, index_to_word))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}